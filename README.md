# GP15_watermassanalysis

This repository stores code for conducting the GP15 water mass analysis described in this manuscript: [https://essopenarchive.org/users/529741/articles/597180](https://essopenarchive.org/users/529741/articles/597180)

Related code is also in the repositories `nitrogenlab/pyompa` and `nitrogenlab/gp15wmascripts`

I (Avanti Shrikumar) have done my best to document key notebooks here. I have used permalinks (i.e. links to a specific git commit) so that the links should still work even if the organization of the repository changes later.

The [pyompa_dev2](https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/6d75edb/pyompa_dev2.ipynb) notebook is a bulky notebook that compiles key results for the thermocline, intermediate and deep waters. You may notice that it has sections for "uncertainty analysis", which didn't make it into the final GP15 manuscript as it explored the spread of end-member fractions present across a space of solutions with slightly worse residuals compared to the optimal solution; however, for the interested reader, the math behind that uncertainty analysis is described in the [PYOMPA preprint](https://essopenarchive.org/doi/full/10.1002/essoar.10507053.4) (sections 3, 3.1 and 3.2).

## Archetype analysis for defining end member subtypes for intermediate and deep waters

"Subtypes" are used in situations where the properties of a particular water mass vary over a wide enough range that it's worth breaking the water mass down into different subtypes rather than trying to come up with a single definition of the water mass properties. The following notebook was used to define the water mass subtypes: [https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/5ee40f0/GP15_Defining_Watermass_Subtypes.ipynb](https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/5ee40f0/GP15_Defining_Watermass_Subtypes.ipynb)

That notebook pulls in GLODAP data (currently just for the pacific and arctic ocean, but can be extended for other regions), and then takes in user-defined limits for the lat/long/density ranges corresponding to the endmembers and spits out a definition for the property values corresponding to each endmember. If you want to use that notebook for your own analysis, the key part of that notebook that you will want to modify is the dictionary `filter_conditions` and the function `filter_rows`. The `filter_conditions` dictionary specifies the names of the water masses as well as the lat/lon/density min/max constraints. The code can easily be extended to have constraints on temperature and salinity (it already has support for constraints on oxygen). Specifically, you would need to change the `filter_rows` function to take in `t_min`, `t_max`, `sal_min`, `sal_max` (it already has support for `ox_min` and `ox_max`), and then also extend the code that filters the data frame to filter on those parameters as well (so add in `df["temperature"] >= t_min`, `df["temperature"] <= t_min`, `df["salinity"] >= sal_min`, `df["salinity"] <= sal_max`). Then, when specifying `filter_conditions`, you would just put in those constraints analogously to the constraints on lat/lon/density. One thing to keep in mind: since the earth is circular, longitude ranges for a water mass can sometimes be discontinuous (e.g. a water mass spanning the pacific may occupy longitudes that are less than -100 OR above +150, but not between -100 to 150); you can set `invert_lon=True` in `filter_conditions` to specify this type of discontinuous longitude range.

The end-member definitions are written to the file `GP15_intermediateanddeep_endmemberswithsubtypes.csv`. As a sanity check, the lat/lon/depth/density observations that were used in the defining of each water type are scatterplotted at the end.

You'll notice that `filter_conditions` also allows you to specify `num_archetypes` for each endmember. `num_archetypes` sets the number of subtypes that will be created for each endmember. The properties for the subtypes are chosen via a process called archetype analysis (described in section 5 of [https://essopenarchive.org/doi/full/10.1002/essoar.10507053.4](https://essopenarchive.org/doi/full/10.1002/essoar.10507053.4)). Basically, it tries to pick an "archetypal" set of property values for the endmember that is representative of the observed spread of property values.

## Performing the analysis for the thermocline

So there are two parts to the thermocline analysis. The first is defining the end member properties for the thermocline, which is done in this notebook: [https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/5233741/ThermoclineEndMemberDefinition.ipynb](https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/5233741/ThermoclineEndMemberDefinition.ipynb). The main difference between what this notebook does and what the other notebook (for intermediate and deep waters) does is that when doing thermocline analysis, you need a different end-member definition for each density stratum (we used a step size of 0.01 to define the strata). Once again, this notebook pulls in GLODAP data (again just for the pacific and arctic ocean). The key thing to focus on is `settings_list`, which specifies the mapping from the end member names to lat/lon/density min/max ranges. The code then pulls in the data for these ranges, and uses cubic smoothing splines to create a mapping from density values to property values (with some outlier filtering added in; I describe the outlier filtering procedure in the section "Text S4" here: [https://essopenarchive.org/doi/full/10.1002/essoar.10510438.1](https://essopenarchive.org/doi/full/10.1002/essoar.10510438.1) - briefly, I compute the standard deviation of the residuals and filter out any points for which the residual exceeds `std_outlier_lim` standard deviations, then repeat the curve-fitting procedure on the non-filtered-out-points, then recompute which points to filter out based on the new curve; this process is iterated until the set of points getting filtered out does not change. Once we have the splines mapping the density to property values, this mapping is used to define the end-member properties for each density range for each end-member. The thermocline endmember definitions are written to files following the naming pattern watertype_name+".csv" (I use "water type" and "endmember" interchangeably).

Some things to keep in mind regarding this thermocline endmember definition notebook. You'll notice the line `for p,std_outlier_lim in [(0.97, np.inf), (0.97, 2), (0.8,np.inf), (0.8, 2)]` - this iterates over different combinations of the parameter `p` and `std_outlier_lim` and runs the function `filter_df_for_range_and_interpolate` for each combo. `p` refers to the level of smoothing used in the cubic smoothing spline (the lower the value, the more the smoothing). `std_outlier_lim` controls how aggressive the outlier filtering is. Every time `filter_df_for_range_and_interpolate` is run, the endmember definitions get overwritten, so it's only the last specified combo that matters (the rest are just there for exploration purposes).

Once you have the end-member definitions for the thermocline, you can refer to the section "Thermocline Analysis" in this notebook for how to use it: [https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/6d75edb/pyompa_dev2.ipynb#scrollTo=BfwymxxTcaoj](https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/6d75edb/pyompa_dev2.ipynb#scrollTo=BfwymxxTcaoj). This notebook pulls in the thermocline endmember definitions, loads them into the object `endmemname_to_df` using the function `read_in_thermocline_endmemberdfs` (which also plots the endmember definitions as a function of density, as a sanity check), and then calls `ThermoclineArrayOMPAProblem` with those definitions. `ThermoclineArrayOMPAProblem` is, fundamentally, a wrapper around `OMPAProblem` that iterates from `tc_lower_bound` to `tc_upper_bound` in increments of `tc_step`, pulls the end-member definitions for the particular density stratum, and tries to explain the observations from that density stratum using those end-member definitions. Some end-members may not be defined for a particular density stratum, but this is fine; the code can handle it (as long as some end-member is defined for that stratum).

The `ThermoclineArrayOMPAProblem` expects the user to filter the observations to exclude those observations that are outside the thermocline. This filtering is done in the following lines, which look complicated primarily because it is applying a different depth cutoff for the thermocline depending on the station number:

```
gp15_thermocline =  gp15_df[gp15_df.apply(
        lambda x: (x['Depth'] > station_to_tcstartend[str(float(x['stnnbr']))]['depth_cutoffs'][0])
              and (x['Depth'] < station_to_tcstartend[str(float(x['stnnbr']))]['depth_cutoffs'][1]), axis=1)]
```

The station-specific cutoffs for where the thermocline starts/ends are stored in `station_to_tcstartend`, which is itself read in from `station_to_tc_cutoffs.json`. The procedure I used for computing these station-specific cutoffs is described in "Text S3" of [https://essopenarchive.org/doi/full/10.1002/essoar.10510438.1](https://essopenarchive.org/doi/full/10.1002/essoar.10510438.1), and is implemented in [https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/7c2ab55/ThermoclineSplittingDev.ipynb](https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/7c2ab55/ThermoclineSplittingDev.ipynb)

## Combined OMPA and OCIM analysis

For the intermediate and deep waters, we followed up the initial (py)OMPA solutions with further using the water mass fractions from an Ocean Circulation Inverse Model (OCIM) to select one unique solution (as there were often multiple equivalent solutions produced by the OMPA due to the relatively large number of end-members considered). A notebook that performs and explains each step in this procedure can be found here: [https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/790c94e/ocimandompa/GP15OCIMandOMPA.ipynb](https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/790c94e/ocimandompa/GP15OCIMandOMPA.ipynb)

Note that the residuals produced by solving for steady-state end-member fractions using the OCIM fluxes alone are quite large because the OCIM fluxes were optimized to produce low residuals when using end members defined in surface gridboxes as the sources, whereas we are using end members defined in intermediate and deep waters as the sources. That is why we performed a combinde OMPA and OCIM analysis.

## How can I set the parameter weightings intelligently?

Unfortunately this part is still more of an art than a science. Here are my thoughts:
- Conceptually, the parameter weightings involve picking *tradeoffs* between which parameters to prioritize fitting. If there is a particular water mass combination that fits all the parameters about as well as they can be fit, then the solver is likely to pick that combination irrespective of the precise values you pick for the parameter weightings
- In line with the observation above, you can (and probably should) do a sensitivity analysis to ensure that your results are not overly dependent on the specific value of your parameters. Here's a notebook where we do that sensitivity analysis: [https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/2555abd/notebooks/sensitivity/SensitivityAnalysis2.ipynb](https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/2555abd/notebooks/sensitivity/SensitivityAnalysis2.ipynb). This notebook randomly varies each parameter weight from 80% to 120% of its original value (the original values are retrieved from gp15wma.settingdefaults.PARAM_WEIGHTINGS), and does that 20 times. It then plots the mean and standard deviation of the water mass fractions across the 20 trials; you can see that the standard deviation was small, and that was how we concluded that the results were largely robust to the parameter weightings.
- My personal advice for setting the parameter weightings would be to follow this process: start by computing what the weightings should be if you were to give all the parameters "effectively equal" weighting. The way to do that would be to assign a weight that is inversely proportional to the spread of the parameter. Technically speaking, one can get into a debate about how to compute "spread", but I think it's fine for these purposes to compute the standard deviation for each parameter in your observational dataset. Having computed these "equal" weights (it doesn't matter what the absolute values are; only the relative values matter), if you have reason to think one parameter should be given less weight than the others (e.g. maybe you want to downweight oxygen given that it may not be conserved due to the presence of biological processes), you can do that downweighting. At this point the process is more art than science, but remember you can always do the sensitivity analysis to add back in the "science".
