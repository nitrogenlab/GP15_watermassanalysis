{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GP15_Defining_Watermass_Subtypes",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitrogenlab/GP15_watermassanalysis/blob/main/GP15_Defining_Watermass_Subtypes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cO6z99Bxu95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9036b86d-528f-4ab7-b0e6-b1c6d7ea4476"
      },
      "source": [
        "!pip install py_pcha\n",
        "!pip install gsw"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting py_pcha\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/02/65048696734e504414b7a8ea171a0c012fcb72daebb69826024478d3a3d8/py_pcha-0.1.3-py3-none-any.whl\n",
            "Installing collected packages: py-pcha\n",
            "Successfully installed py-pcha-0.1.3\n",
            "Collecting gsw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/33/c3ec417e4cd1cc7f8039ce75f92f67010cca7f46a505906e32be2b33f008/gsw-3.4.0-cp37-cp37m-manylinux2010_x86_64.whl (2.4MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4MB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gsw) (1.19.5)\n",
            "Installing collected packages: gsw\n",
            "Successfully installed gsw-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qqA6m0COfsu"
      },
      "source": [
        "Grab the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v0JbRw9rGjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb389bec-b037-4d61-8b1e-535dd634e9ad"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1E9XGag2_uC2TM_5DcOcmSz86I1xj6hHr' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1E9XGag2_uC2TM_5DcOcmSz86I1xj6hHr\" -O GLODAPv2.2019_Pacific_Ocean.csv && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-07 16:51:01--  https://docs.google.com/uc?export=download&confirm=o&id=1E9XGag2_uC2TM_5DcOcmSz86I1xj6hHr\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.119.113, 108.177.119.138, 108.177.119.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘GLODAPv2.2019_Pacific_Ocean.csv’\n",
            "\n",
            "GLODAPv2.2019_Pacif     [ <=>                ]   3.22K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-07 16:51:01 (42.6 MB/s) - ‘GLODAPv2.2019_Pacific_Ocean.csv’ saved [3301]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4pyP4Wdh525"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas\n",
        "import gsw"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI2Ec0E-P4z2"
      },
      "source": [
        "Read in the data frame and pick a subset of columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0XJMvR2ix85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59c97f24-9644-4d44-d923-b6c6b0ba91d9"
      },
      "source": [
        "colnames_subset = [\"latitude\", 'longitude', 'year', 'depth',\n",
        "                   'pressure', 'temperature','salinity','oxygen',\n",
        "                   'silicate', 'nitrate', 'phosphate', 'sigma0']\n",
        "\n",
        "\n",
        "df = pandas.read_csv(\"GLODAPv2.2019_Pacific_Ocean.csv\",\n",
        "                     na_values = -9999)[colnames_subset]\n",
        "\n",
        "df['NO'] = df['oxygen'] + (df['nitrate']*9.68)\n",
        "df['PO'] = df['oxygen'] + (df['phosphate']*155)\n",
        "df['potential_temperature'] = gsw.pt_from_t(df['salinity'],\n",
        "                                df['temperature'],\n",
        "                                df['pressure'],\n",
        "                                df['sigma0'])\n",
        "df[\"absolute_salinity\"] = gsw.SA_from_SP(\n",
        "    SP=df[\"salinity\"], p=df[\"pressure\"],\n",
        "    lon=df[\"longitude\"], lat=df[\"latitude\"])\n",
        "df[\"conservative_temperature\"] = gsw.CT_from_t(SA=df[\"absolute_salinity\"],\n",
        "                                  t=df[\"temperature\"],\n",
        "                                  p=df[\"pressure\"])\n",
        "df[\"sigma2\"] = gsw.sigma2(SA=df[\"absolute_salinity\"],CT=df[\"conservative_temperature\"])\n",
        "df[\"sigma4\"] = gsw.sigma4(SA=df[\"absolute_salinity\"],CT=df[\"conservative_temperature\"])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-388e78a60098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m df = pandas.read_csv(\"GLODAPv2.2019_Pacific_Ocean.csv\",\n\u001b[0;32m----> 7\u001b[0;31m                      na_values = -9999)[colnames_subset]\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NO'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'oxygen'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nitrate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m9.68\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['latitude', 'longitude', 'year', 'depth', 'pressure', 'temperature',\\n       'salinity', 'oxygen', 'silicate', 'nitrate', 'phosphate', 'sigma0'],\\n      dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Sm_Pf1o1bjH"
      },
      "source": [
        "#ARCH_ANALYSIS_FEATURES = [\"conservative_temperature\", \"absolute_salinity\",\n",
        "#                          \"silicate\", \"NO\", \"PO\"]\n",
        "ARCH_ANALYSIS_FEATURES = [\"conservative_temperature\", \"absolute_salinity\",\n",
        "                          \"silicate\", \"nitrate\", \"phosphate\", \"oxygen\"]\n",
        "ALL_FEATURES_TO_AVERAGE = ARCH_ANALYSIS_FEATURES#+[\"phosphate\", \"nitrate\", \"oxygen\"]\n",
        "\n",
        "#feature_weights = {\"conservative_temperature\": 56.0,\n",
        "#                   \"absolute_salinity\": 80.0,\n",
        "#                   \"silicate\": 3.0,\n",
        "#                   \"NO\": 5.0,\n",
        "#                   \"PO\": 5.0}\n",
        "feature_weights = {\"conservative_temperature\": 5.0,\n",
        "                   \"absolute_salinity\": 5.0,\n",
        "                   \"silicate\": 1.0,\n",
        "                   \"NO\": 1.0,\n",
        "                   \"PO\": 1.0,\n",
        "                   \"nitrate\": 1.0,\n",
        "                   \"phosphate\": 1.0,\n",
        "                   \"oxygen\": 1.0}\n",
        "\n",
        "print(\"Standardizing feature values\")\n",
        "#Let's standardize each column by subtracting mean,\n",
        "# dividing by standard deviation and multiplying by the feature weights.\n",
        "# Call it a 'features' dataframe\n",
        "#keep track of mean and std in order to do inverse transform\n",
        "colname_to_mean = {}\n",
        "colname_to_std = {} \n",
        "for colname in ARCH_ANALYSIS_FEATURES:\n",
        "  vals = np.array(df[colname])\n",
        "  #use nanmean and nanstd to ignore nan values for now\n",
        "  mean = np.nanmean(vals)\n",
        "  std = np.nanstd(vals)\n",
        "  colname_to_mean[colname] = mean\n",
        "  colname_to_std[colname] = std\n",
        "  df['standardized_'+colname] = feature_weights[colname]*(vals-mean)/std\n",
        "\n",
        "#Since imputation takes a while on such a large dataset, we\n",
        "# will just drop rows that have missing values\n",
        "print(\"Proportions of missing values:\")\n",
        "print(np.sum(np.isnan(df)) / len(df))\n",
        "print(\"Original number of rows:\", len(df))\n",
        "df = df.dropna()\n",
        "print(\"Remaining rows after dropping missing vals:\",len(df))\n",
        "\n",
        "TRANSFORM_MEANS = np.array([colname_to_mean[colname]\n",
        "                           for colname in ARCH_ANALYSIS_FEATURES])\n",
        "TRANSFORM_STDS = np.array([(colname_to_std[colname]/feature_weights[colname])\n",
        "                           for colname in ARCH_ANALYSIS_FEATURES])\n",
        "\n",
        "\n",
        "#function to map features back to original space\n",
        "def map_features_back(features):\n",
        "  return features*TRANSFORM_STDS[None,:] + TRANSFORM_MEANS[None,:]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5hNQdcE2tnn"
      },
      "source": [
        "from py_pcha import PCHA\n",
        "import scipy \n",
        "from scipy import spatial\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n",
        "def flag_outliers(features, n_neighbors=20, zscore_threshold=2):\n",
        "    #Features is observations X features\n",
        "    #Do a process of removing outliers; if the average distance of a point to\n",
        "    # its nearest N neighbors is more than stdev number of standard deviations from\n",
        "    # the mean, flag it as an outlier.\n",
        "    #I tried the Extreme Studentized Deviate but it wasn't behaving very\n",
        "    # predictably. I guess the distribution of neighbor distances is\n",
        "    # exponential-ish rather than normal.\n",
        "    pairwise_distances = scipy.spatial.distance.squareform(\n",
        "        scipy.spatial.distance.pdist(X=features, metric=\"euclidean\"))\n",
        "    avg_dist_to_nn = np.mean(\n",
        "        np.sort(pairwise_distances, axis=-1)[:,1:n_neighbors+1], axis=-1)\n",
        "    zscore_avgdisttonn = (avg_dist_to_nn - np.mean(avg_dist_to_nn))/np.std(\n",
        "                          avg_dist_to_nn)\n",
        "    #plt.hist(zscore_avgdisttonn, bins=100)\n",
        "    #plt.show()\n",
        "    anomalous_indices = np.abs(zscore_avgdisttonn) > zscore_threshold\n",
        "    return anomalous_indices\n",
        "\n",
        "\n",
        "def run_archetype_analysis(df, num_endmembers):\n",
        "    all_features = np.array([np.array(df[\"standardized_\"+col])\n",
        "                      for col in ARCH_ANALYSIS_FEATURES]).transpose((1,0))\n",
        "    all_origspace_features = np.array([np.array(df[col]) for col in\n",
        "                                   ALL_FEATURES_TO_AVERAGE]).transpose((1,0))\n",
        "\n",
        "    outliers = flag_outliers(all_features)\n",
        "    features = all_features[outliers==False]\n",
        "    origspace_features = all_origspace_features[outliers==False]\n",
        "\n",
        "    XC, S, C, SSE, varexpl = PCHA(X=features.T, noc=num_endmembers,\n",
        "                                  verbose=False)\n",
        "    archetype_features = np.array(XC).T #becomes archtetypes X features\n",
        "    obs_combos_giving_archetypes = np.array(C).T #archetypes X obs combos\n",
        "\n",
        "    origspace_archetypes = obs_combos_giving_archetypes@origspace_features\n",
        "\n",
        "    #Make scatterplots\n",
        "    fig,ax = plt.subplots(nrows=1, ncols=len(ARCH_ANALYSIS_FEATURES)-1,\n",
        "                          figsize=(25,5))\n",
        "    pltnum = 0\n",
        "    for featureidx1 in [0]:#range(len(ALL_FEATURES_TO_AVERAGE)):\n",
        "        for featureidx2 in range(featureidx1+1, len(ALL_FEATURES_TO_AVERAGE)):\n",
        "            if ((ALL_FEATURES_TO_AVERAGE[featureidx1]\n",
        "                 in ARCH_ANALYSIS_FEATURES) and\n",
        "                (ALL_FEATURES_TO_AVERAGE[featureidx2]\n",
        "                 in ARCH_ANALYSIS_FEATURES)):\n",
        "                plt.sca(ax[pltnum])\n",
        "                plt.scatter(all_origspace_features[:,featureidx1],\n",
        "                            all_origspace_features[:,featureidx2],\n",
        "                            c=[(\"red\" if x==True else \"C0\") for x in outliers],\n",
        "                            s=1)\n",
        "                plt.scatter(origspace_archetypes[:,featureidx1],\n",
        "                            origspace_archetypes[:,featureidx2],\n",
        "                            color=\"C1\")\n",
        "                plt.xlabel(ALL_FEATURES_TO_AVERAGE[featureidx1])\n",
        "                plt.ylabel(ALL_FEATURES_TO_AVERAGE[featureidx2]) \n",
        "                pltnum += 1\n",
        "    plt.show()\n",
        "\n",
        "    return pandas.DataFrame(dict([(colname, origspace_archetypes[:,idx])\n",
        "                        for idx,colname in enumerate(ALL_FEATURES_TO_AVERAGE)]))\n",
        "    \n",
        "\n",
        "filter_conditions = {\n",
        "    \"AAIW\": {\"lat_min\":-51.0, \"lat_max\":-47.0,\n",
        "             \"lon_min\":-120.0, \"lon_max\":150.0,\n",
        "             \"sig0_min\":27.01, \"sig0_max\":27.1},\n",
        "    \"NPIW\": {\"lat_min\":36.0, \"lat_max\":40.0, \n",
        "             \"lon_min\":150.0, \"lon_max\":170.0,\n",
        "             \"sig0_min\":26.6, \"sig0_max\":27.0,\n",
        "             \"ox_min\":0, \"ox_max\":150.0},\n",
        "    \"UCDW\": {\"lat_min\":-50.0, \"lat_max\":-45.0, \n",
        "             \"lon_min\":-120.0, \"lon_max\":150.0,\n",
        "             \"sig2_min\":36.96, \"sig2_max\":200,\n",
        "             \"sig4_min\":0, \"sig4_max\":45.84),\n",
        "    \"LCDW\": {\"lat_min\":-64.0, \"lat_max\":-58.0, \n",
        "             \"lon_min\":-120.0, \"lon_max\":150.0,\n",
        "             \"sig4_min\":45.84, \"sig4_max\":46.04},\n",
        "    \"AABW\": {\"lat_min\":-70.0, \"lat_max\":-60.0, \n",
        "             \"lon_min\":-45.0, \"lon_max\":-10.0,\n",
        "             \"sig4_min\":46.04, \"sig4_max\":200},\n",
        "    \"PDW1\": {\"lat_min\":39.0, \"lat_max\":51.0, \n",
        "             \"lon_min\":-170.0, \"lon_max\":-133.0,\n",
        "             \"sig0_min\":27.6, \"sig0_max\":200},\n",
        "             \"sig2_min\":0, \"sig2_max\":36.96},\n",
        "    \"PDW2-3\": {\"lat_min\":39.0, \"lat_max\":51.0, \n",
        "             \"lon_min\":-170.0, \"lon_max\":-133.0,\n",
        "             \"sig2_min\":36.96, \"sig2_max\":200,\n",
        "             \"sig4_min\":0, \"sig4_max\":45.88}\n",
        "    \n",
        "}\n",
        "\n",
        "def filter_rows(df, lat_min, lat_max, lon_min, lon_max, sig0_min=0, sig0_max=np.inf, \n",
        "                sig2_min=0, sig2_max=np.inf, sig4_min=0, sig4_max=np.inf, ox_min=0, ox_max=np.inf):\n",
        "  return df[(df[\"latitude\"] >= lat_min) &\n",
        "            (df[\"latitude\"] <= lat_max) &\n",
        "            (df[\"longitude\"] >= lon_min) &\n",
        "            (df[\"longitude\"] <= lon_max) &\n",
        "            (df[\"sigma0\"] >= sig0_min) &\n",
        "            (df[\"sigma0\"] <= sig0_max) &\n",
        "            (df[\"sigma2\"] >= sig2_min) &\n",
        "            (df[\"sigma2\"] <= sig2_max) &\n",
        "            (df[\"sigma4\"] >= sig4_min) &\n",
        "            (df[\"sigma4\"] <= sig4_max) &\n",
        "            (df[\"oxygen\"] >= ox_min) &\n",
        "            (df[\"oxygen\"] <= ox_max) \n",
        "            ]\n",
        "\n",
        "NUM_ARCHETYPES = 4\n",
        "\n",
        "watermass_to_archetypes = {}\n",
        "\n",
        "for watermass in filter_conditions:\n",
        "  print(\"On water mass\", watermass)\n",
        "  print(\"Ranges:\", filter_conditions[watermass])\n",
        "  watermass_rows = filter_rows(df=df, **filter_conditions[watermass])\n",
        "  watermass_archetypes = run_archetype_analysis(df=watermass_rows,\n",
        "                                                num_endmembers=NUM_ARCHETYPES)\n",
        "  watermass_to_archetypes[watermass] = watermass_archetypes\n",
        "  display(watermass_archetypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7PfaqTTf9xd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}